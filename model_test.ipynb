{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0581a022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from transformers import AutoModel\n",
    "from mert_fairseq.models.mert import MERTConfig, MERTModel\n",
    "from mert_fairseq.tasks.mert_pretraining import MERTPretrainingTask\n",
    "import torch\n",
    "import yaml\n",
    "from torch import nn\n",
    "import torchaudio.transforms as T\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import inspect\n",
    "from fairseq import checkpoint_utils, data, options, tasks\n",
    "from fairseq.data.data_utils import compute_mask_indices\n",
    "from timm.models.vision_transformer import Block, PatchEmbed\n",
    "from omegaconf import OmegaConf\n",
    "import torchaudio\n",
    "from nnAudio.features.mel import MelSpectrogram\n",
    "\n",
    "from musiceval.tasks import GTZAN\n",
    "from mert_fairseq.models.mert import model_cqt_pred, MERTModel, MERTConfig\n",
    "from mert_fairseq.models.mert.utils import get_1d_sincos_pos_embed\n",
    "from mert_fairseq.tasks.mert_pretraining import MERTPretrainingTask, MERTPretrainingConfig\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba0194f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mert_fairseq.tasks.mert_pretraining:current directory is /data/home/acw713/workspace/mert/MERT\n",
      "INFO:mert_fairseq.tasks.mert_pretraining:MERTPretrainingTask Config MERTPretrainingConfig(_name='mert_pretraining', data='data/fma_medium', sharding_data=-1, load_random_data_shard=True, fine_tuning=False, labels=['encodec_0'], label_dir='data/encodec_labels/fma_medium', label_rate=75.0, sample_rate=24000, normalize=False, enable_padding=False, max_keep_size=None, max_sample_size=120000, min_sample_size=72000, single_target=False, random_crop=True, pad_audio=False, store_labels=False, numpy_memmap_label=False, augmentation_effects='[]', augmentation_probs='[]', inbatch_noise_augment_len_range='[8000, 24000]', inbatch_noise_augment_number_range='[1, 3]', inbatch_noise_augment_volume=1.0, dynamic_crops='[]', dynamic_crops_epoches='[]', cqt_loss_bin_dataloader=-1)\n",
      "INFO:mert_fairseq.models.mert.mert_model:MERTModel Config: MERTConfig(_name='mert', label_rate=75.0, extractor_mode='default', encoder_layers=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_attention_heads=12, activation_fn='gelu', layer_type='transformer', dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, encoder_layerdrop=0.05, dropout_input=0.1, dropout_features=0.1, final_dim=64, untie_final_proj=True, layer_norm_first=False, audio_extract_type='w2v_conv', music_conv_nmel=80, music_conv_hoplen=40, conv_feature_layers='[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', conv_bias=False, logit_temp=0.1, target_glu=False, feature_grad_mult=0.1, conv_pos=128, conv_pos_groups=16, pos_conv_depth=1, mask_type='mae', mask_length=5, mask_prob=0.8, mask_dynamic_prob_step='[]', mask_dynamic_prob='[]', mask_dynamic_len_step='[]', mask_dynamic_len='[]', mask_selection='static', mask_other=0, no_mask_overlap=False, mask_min_space=1, mask_replace=0.0, mask_replace_type='in_sample', mask_origin=0.0, mask_channel_length=10, mask_channel_prob=0.0, mask_channel_selection='static', mask_channel_other=0, no_mask_channel_overlap=False, mask_channel_min_space=1, latent_temp=(2, 0.5, 0.999995), skip_masked=False, skip_nomask=True, checkpoint_activations=False, required_seq_len_multiple=2, depthwise_conv_kernel_size=31, attn_type='', pos_enc_type='abs', fp16=False, audio_cqt_loss_m=True, audio_cqt_bins=336, audio_mel_loss_m=False, audio_mel_bins=128, feature_extractor_cqt=False, feature_extractor_cqt_bins=84, mixture_prob=0.5, inbatch_noise_augment_len_range='[12000, 24000]', inbatch_noise_augment_number_range='[1, 3]', inbatch_noise_augment_volume=1.0, learnable_temp=False, learnable_temp_init=0.1, learnable_temp_max=100.0, chunk_nce_cal=-1, pretrained_weights='', random_codebook=-1, deepnorm=False, subln=False, emb_grad_mult=1.0, attention_relax=-1.0, do_cnn_feat_stable_layernorm=False, wav_normalize=False, logistic_cqt=False)\n",
      "INFO:mert_fairseq.models.mert.mert_model:train the model with extra task: reconstruct cqt from transformer output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CQT kernels created, time used = 1.1683 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/acw713/.conda/envs/map/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "INFO:mert_fairseq.data.mert_dataset:max_keep=None, min_keep=72000, loaded 24, skipped 0 short and 0 long, longest-loaded=720065, shortest-loaded=719438\n",
      "INFO:mert_fairseq.data.mert_dataset:preparing labels\n",
      "24it [00:00, 17592.33it/s]\n",
      "INFO:mert_fairseq.data.mert_dataset:pad_audio=False, random_crop=True, normalize=False, max_sample_size=120000\n",
      "INFO:fairseq.tasks.fairseq_task:can_reuse_epoch_itr = False\n",
      "INFO:fairseq.tasks.fairseq_task:reuse_dataloader = True\n",
      "INFO:fairseq.tasks.fairseq_task:rebuild_batches = False\n",
      "INFO:fairseq.tasks.fairseq_task:creating new batches for epoch 1\n"
     ]
    }
   ],
   "source": [
    "with open(\"mert_fairseq/config/pretrain/MERT_RVQ-VAE_CQT_95M.yaml\", \"r\") as f:\n",
    "    yaml_dict = yaml.safe_load(f)\n",
    "\n",
    "cfg = OmegaConf.create(yaml_dict)\n",
    "cfg['model']['label_rate'] = 75.\n",
    "cfg['model']['mask_type'] = \"mae\" \n",
    "# cfg['model']['audio_extract_type'] = \"spec_mlp\" \n",
    "cfg['model']['audio_mel_bins'] = 128\n",
    "cfg[\"task\"][\"data\"] = 'data/fma_medium'\n",
    "cfg[\"task\"][\"label_dir\"] = 'data/encodec_labels/fma_medium'\n",
    "cfg[\"task\"][\"labels\"] = [\"encodec_0\"]\n",
    "\n",
    "model_cfg = MERTConfig(**cfg[\"model\"])\n",
    "task_cfg = MERTPretrainingConfig(**cfg[\"task\"])\n",
    "task = MERTPretrainingTask(task_cfg)\n",
    "model = MERTModel(model_cfg, task_cfg, task.dictionaries)\n",
    "task.load_dataset(\"valid\")\n",
    "batch_iter = task.get_batch_iterator(\n",
    "    dataset=task.dataset(\"valid\"),\n",
    "    max_tokens=120000*5,\n",
    "    ignore_invalid_inputs=True,\n",
    "    num_workers=0,\n",
    "    epoch=1,\n",
    ").next_epoch_itr(shuffle=False)\n",
    "batch = next(batch_iter)\n",
    "input = batch[\"net_input\"]['source']\n",
    "padding_mask = batch[\"net_input\"]['padding_mask']\n",
    "target_list = batch[\"target_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b688e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 128, 376])\n",
      "torch.Size([5, 192, 512])\n",
      "torch.Size([5, 38, 768]) tensor([5, 4, 4, 3, 4, 5, 4, 4, 3, 3, 5, 3, 5, 4, 2, 4, 4, 5, 4, 5, 5, 3, 3, 5,\n",
      "        4, 5, 3, 4, 5, 5, 3, 5, 5, 4, 1, 4, 4, 2, 4, 3, 4, 3, 4, 3, 4, 3, 5, 4,\n",
      "        5, 4, 3, 4, 3, 5, 5, 4, 4, 3, 4, 3, 4, 4, 4, 3, 5, 4, 5, 5, 5, 4, 4, 5,\n",
      "        4, 3, 4, 4, 2, 5, 5, 3, 5, 4, 4, 5, 4, 5, 4, 5, 5, 4, 4, 4, 5, 5, 3, 4,\n",
      "        4, 2, 2, 5, 5, 5, 5, 3, 5, 3, 4, 5, 5, 4, 4, 5, 5, 4, 4, 3, 4, 4, 5, 4,\n",
      "        3, 3, 4, 4, 5, 5, 4, 5, 3, 4, 2, 5, 4, 5, 4, 5, 5, 4, 5, 3, 3, 5, 4, 4,\n",
      "        3, 5, 5, 4, 3, 2, 3, 4, 4, 5, 5, 5, 5, 3, 1, 5, 4, 4, 1, 5, 3, 3, 4, 5,\n",
      "        4, 4, 4, 5, 5, 5, 4, 4, 4, 4, 3, 4, 4, 4, 5, 5, 2, 4, 4, 3, 3, 4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "loss_dict = model.forward(input, target_list=target_list, padding_mask=padding_mask, mask=True, features_only=False)\n",
    "# x = feature_dict['x']\n",
    "# masked_indices = feature_dict['masked_indices']\n",
    "# id_restore = feature_dict['id_restore']\n",
    "# x.shape, masked_indices.shape, id_restore.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f91ed1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([770, 1029])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_dict[\"logit_m_list\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a6b7541",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/acw713/workspace/mert/MERT/src/fairseq/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(f, map_location=torch.device(\"cpu\"))\n",
      "INFO:mert_fairseq.tasks.mert_pretraining:current directory is /data/home/acw713/workspace/mert/MERT\n",
      "INFO:mert_fairseq.tasks.mert_pretraining:MERTPretrainingTask Config {'_name': 'mert_pretraining', 'data': '/data/home/acw713/workspace/mert/MERT/data/fma4all', 'sharding_data': -1, 'load_random_data_shard': True, 'fine_tuning': False, 'labels': ['encodec_0', 'encodec_1', 'encodec_2', 'encodec_3', 'encodec_4', 'encodec_5', 'encodec_6', 'encodec_7'], 'label_dir': '/data/home/acw713/workspace/mert/MERT/data/encodec_labels/fma4all', 'label_rate': 75.0, 'sample_rate': 24000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 120000, 'min_sample_size': 72000, 'single_target': False, 'random_crop': True, 'pad_audio': False, 'store_labels': False, 'numpy_memmap_label': False, 'augmentation_effects': '[]', 'augmentation_probs': '[]', 'inbatch_noise_augment_len_range': '[8000, 24000]', 'inbatch_noise_augment_number_range': '[1, 3]', 'inbatch_noise_augment_volume': 1.0, 'dynamic_crops': '[]', 'dynamic_crops_epoches': '[]', 'cqt_loss_bin_dataloader': -1}\n",
      "INFO:mert_fairseq.models.mert.mert_model:MERTModel Config: {'_name': 'mert', 'label_rate': 75.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 64, 'untie_final_proj': True, 'layer_norm_first': True, 'audio_extract_type': w2v_conv, 'music_conv_nmel': 80, 'music_conv_hoplen': 40, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'mask_type': 'hubert', 'mask_length': 5, 'mask_prob': 0.8, 'mask_dynamic_prob_step': '[]', 'mask_dynamic_prob': '[]', 'mask_dynamic_len_step': '[]', 'mask_dynamic_len': '[]', 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_replace': 0.0, 'mask_replace_type': in_sample, 'mask_origin': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': True, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'audio_cqt_loss_m': True, 'audio_cqt_bins': 336, 'audio_mel_loss_m': False, 'audio_mel_bins': 84, 'feature_extractor_cqt': False, 'feature_extractor_cqt_bins': 84, 'mixture_prob': 0.5, 'inbatch_noise_augment_len_range': '[12000, 24000]', 'inbatch_noise_augment_number_range': '[1, 3]', 'inbatch_noise_augment_volume': 1.0, 'learnable_temp': False, 'learnable_temp_init': 0.1, 'learnable_temp_max': 100.0, 'chunk_nce_cal': -1, 'pretrained_weights': '', 'random_codebook': -1, 'deepnorm': False, 'subln': False, 'emb_grad_mult': 1.0, 'attention_relax': -1.0, 'do_cnn_feat_stable_layernorm': False, 'wav_normalize': False, 'logistic_cqt': True}\n",
      "/data/home/acw713/.conda/envs/map/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "INFO:mert_fairseq.models.mert.mert_model:train the model with extra task: reconstruct cqt from transformer output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CQT kernels created, time used = 1.5077 seconds\n"
     ]
    }
   ],
   "source": [
    "# ckpts = checkpoint_utils.load_model_ensemble_and_task([\"/homes/nw003/workspace/stage0/MERT/models/MERT-v1-95M_fairseq.pt\"])\n",
    "ckpt_path = \"/data/scratch/acw713/mert/results/25-06-23-logistic-float16-continuous/ckpt_MERT_RVQ-VAE_CQT/MERT_RVQ-VAE_CQT_95M/checkpoint_last.pt\"\n",
    "models, cfg, task = checkpoint_utils.load_model_ensemble_and_task([ckpt_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb1fd5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MERTModel(\n",
       "  (feature_extractor): ConvFeatureExtractionModel(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        (3): GELU(approximate='none')\n",
       "      )\n",
       "      (1-4): 4 x Sequential(\n",
       "        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): GELU(approximate='none')\n",
       "      )\n",
       "      (5-6): 2 x Sequential(\n",
       "        (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)\n",
       "  (dropout_input): Dropout(p=0.1, inplace=False)\n",
       "  (dropout_features): Dropout(p=0.1, inplace=False)\n",
       "  (encoder): TransformerEncoder(\n",
       "    (pos_conv): Sequential(\n",
       "      (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "      (1): SamePad()\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (final_proj): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (encoder_cqt_model): model_cqt_pred(\n",
       "    (spec_layer): CQT()\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=336, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm1d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv): Sequential(\n",
       "      (0): Conv1d(1, 30, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "    (target_scaler): Scaler()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models[0]\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dd02a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, sr = torchaudio.load(\"/data/scratch/acw713/datasets/fma4all/wav/107036.wav\")\n",
    "audio = torchaudio.functional.resample(audio, orig_freq=sr, new_freq=24000)[:, :120000*5]\n",
    "sources = torch.chunk(audio, 5, dim=1)\n",
    "sources = torch.cat(sources, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c659d8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "x = model.extract_features(sources, ret_layer=False)\n",
    "cqt_pred_m = model.encoder_cqt_model(x)\n",
    "source_cqt = model.encoder_cqt_model.compute_cqt(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e1ebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cqt_model = model_cqt_pred(768, 336, 24000, 75, logistic=True)\n",
    "cqt_pred_m = cqt_model(x[mask_indices], forward_type=\"masked_logistic_output\")\n",
    "source_cqt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b91d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_cqt = source_cqt[:mask_indices.shape[0], :mask_indices.shape[1]]\n",
    "loss = cqt_model.criterion(cqt_pred_m, source_cqt[mask_indices])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35cc617",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cqt_model.target_scaler(source_cqt[mask_indices])\n",
    "data.min(), data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc04091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(cqt_pred_m[1].detach().numpy().T, origin='lower', aspect='auto', vmin=0, vmax=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abe4dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(source_cqt[1].detach().numpy().T, origin='lower', aspect='auto', vmin=0, vmax=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641e468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtzan = GTZAN(\"/import/c4dm-05/nw003/datasets/gtzan\", segment_length=5, sample_rate=24000, duration=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9e445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtzan_dataset = gtzan.dataset(transform=\"AudioFeature\", preload=\"./test_data\", encoder=model, trainset_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e75eadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtzan_dataset.setup(\"fit\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d10ceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(gtzan_dataset.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9884fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b54a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "map",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
